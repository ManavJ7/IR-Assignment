{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR_Group18.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Maq343IhGyIe"
      },
      "source": [
        "***Group Number 18***\n",
        "\n",
        "Group Members\n",
        "\n",
        "Manav Jain - 2018A7PS0102G\n",
        "\n",
        "Rohan Agarwal - 2018A7PS0123G\n",
        "\n",
        "Mane Digvijaysinh Sujaysinh - 2018A7PS0093G\n",
        "\n",
        "Vishisht Agarwal - 2018A7PS0224G"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv1wp-Ov0wA2"
      },
      "source": [
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fImVmSIe29YV"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2iJQ-pD2_WG"
      },
      "source": [
        "document=open(\"/content/drive/My Drive/IR_assign/documents.txt\", 'r')\n",
        "relevance_assessment=open(\"/content/drive/My Drive/IR_assign/relevance_assessment.txt\", 'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-zJKrjbFQPG"
      },
      "source": [
        "## **TASK 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FDLMoLIEdKO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUz7uRezt0ZE"
      },
      "source": [
        "PARSING DOCUMENTS FROM QUERIES\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C17bXida3jGN"
      },
      "source": [
        "docs=document.read()\n",
        "relevant = relevance_assessment.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or4B02F-n_Ta"
      },
      "source": [
        "docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2y0bqmS3ra_"
      },
      "source": [
        "p=docs.split('/')\n",
        "p = [x.replace(\"\\n\",\" \") for x in p]\n",
        "p=[x.strip() for x in p]\n",
        "p.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqHbnPoRoEre"
      },
      "source": [
        "p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dUZqm_srTbs"
      },
      "source": [
        "type(relevant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_raZBD4Xva6C"
      },
      "source": [
        "r=relevant.split('/')\n",
        "r = [x.replace(\"\\n\",\" \") for x in r]\n",
        "r=[x.strip() for x in r]\n",
        "r.pop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEVnXmeToJpG"
      },
      "source": [
        "r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO0Rgmfm5omH"
      },
      "source": [
        "relevant_dict = {}\n",
        "relevant_docs = []\n",
        "for s in r:\n",
        "  t1 = s.split() \n",
        "  t2 = t1[1:]\n",
        "  relevant_docs.append(t2) \n",
        "  for i in t2:\n",
        "    relevant_dict[i]=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgJrkl0QVSJW"
      },
      "source": [
        "relevant_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "royOJTcfrd2l"
      },
      "source": [
        "relevant_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll_n_oRmv_m9"
      },
      "source": [
        "documents={}\n",
        "for s in p:\n",
        "  t1 = s.split() \n",
        "  t2 = t1[1:] \n",
        "  s2 = ' '.join(t2)\n",
        "  if (t1[0] in relevant_dict.keys()):\n",
        "      documents[int(t1[0])] = s2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1VFbUUooNDC"
      },
      "source": [
        "documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCElyj30trsd"
      },
      "source": [
        "INVERTED INDEX\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju3aZe43sUQO"
      },
      "source": [
        "class getInvertedIndex(object):\n",
        "  def __init__(self,docs):\n",
        "    self.docs = docs\n",
        "    self.termList = []\n",
        "    self.docLists = []\n",
        "    self.final_dict = {}\n",
        "\n",
        "    for index in docs.keys():\n",
        "      for term in docs[index].split(\" \"):\n",
        "        if term in self.termList:\n",
        "          i=self.termList.index(term)\n",
        "          if (index) not in self.docLists[i]:\n",
        "            self.docLists[i].append(index)\n",
        "        else:\n",
        "          self.termList.append(term)\n",
        "          self.docLists.append([index])\n",
        "    inverted_dict={}\n",
        "    for terms in range(len(self.termList)):\n",
        "      inverted_dict[self.termList[terms]] = self.docLists[terms]\n",
        "    dict_len= {key: len(value) for key, value in inverted_dict.items()}\n",
        "    import operator\n",
        "    sorted_key_list = sorted(dict_len.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    sorted_dict = [{item[0]: inverted_dict[item [0]]} for item in sorted_key_list]\n",
        "    for i in range(len(sorted_dict)):\n",
        "      for j in sorted_dict[i]:\n",
        "        self.final_dict[j] = sorted_dict[i][j]\n",
        "     \n",
        "  def search(self,term):\n",
        "    try:\n",
        "      i=self.termList.index(term)\n",
        "      return self.docLists[i]\n",
        "    except:\n",
        "      return \"No results\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FWEljUztO2j"
      },
      "source": [
        "myInvertedIndex=getInvertedIndex(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVNC4-7Hgt4F"
      },
      "source": [
        "myInvertedIndex.final_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpPhjH-5uLXQ"
      },
      "source": [
        "VOCAB SIZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8szYcTpuNeK"
      },
      "source": [
        "len (myInvertedIndex.termList)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnX76SwSuR7N"
      },
      "source": [
        "PARSING QUERY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z-DFrmauRPU"
      },
      "source": [
        "queryDoc=open(\"/content/drive/My Drive/IR_assign/query.txt\", 'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeVYDR07v4Fh"
      },
      "source": [
        "queryInit=queryDoc.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2tiMsI9wISp"
      },
      "source": [
        "queryFin=queryInit.split('/')\n",
        "queryFin = [x.replace(\"\\n\",\" \") for x in queryFin]\n",
        "queryFin=[x.strip() for x in queryFin]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L8AzAGMxjj0"
      },
      "source": [
        "originalQueries={}\n",
        "for i in range(len(queryFin)-1):\n",
        "  s = queryFin[i]\n",
        "  t1 = s.split()\n",
        "  t2 = t1[1:] \n",
        "  s2 = ' '.join(t2)\n",
        "  s2 = s2.lower();\n",
        "  t2 = s2.split();\n",
        "  originalQueries[int(t1[0])]=[]\n",
        "  for idx in range(len(t2)):\n",
        "    originalQueries[int(t1[0])].append(t2[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmYfeyK0hdwu"
      },
      "source": [
        "type(originalQueries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyP6g4lU4GsN"
      },
      "source": [
        "queries={}\n",
        "for i in range(1,11):\n",
        "  queries[i]=[]\n",
        "  for word in originalQueries[i]:\n",
        "      queries[i].append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBLeoDbVhYJk"
      },
      "source": [
        "queries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VTeXQz935X-"
      },
      "source": [
        "PROCESSING 'OR' QUERY\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY1qGXJ84BbC"
      },
      "source": [
        "def orQuery(list1, list2):\n",
        "  ans=[]\n",
        "  p1=0\n",
        "  p2=0\n",
        "  while (p1<len(list1) and p2<len(list2)):\n",
        "    if (list1[p1] == list2[p2]):\n",
        "      ans.append(list1[p1])\n",
        "      p1=p1+1\n",
        "      p2=p2+1\n",
        "    else:\n",
        "      if list1[p1] > list2[p2]:\n",
        "        ans.append(list2[p2])\n",
        "        p2=p2+1\n",
        "      else:\n",
        "        ans.append(list1[p1])\n",
        "        p1=p1+1\n",
        "  while p1<len(list1):\n",
        "    ans.append(list1[p1])\n",
        "    p1=p1+1\n",
        "  while p2<len(list2):\n",
        "    ans.append(list2[p2])\n",
        "    p2=p2+1\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHRzeEiS-2ht"
      },
      "source": [
        "RETRIEVING DOCUMENTS WITHOUT ANY LINGUISTIC MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9qTU4Dz6JzR"
      },
      "source": [
        "docsRetrieved={}\n",
        "for q in range(1,11):\n",
        "  docsRetrieved[q]=[]\n",
        "  for word in queries[q]:\n",
        "    if word in myInvertedIndex.final_dict:\n",
        "      docsRetrieved[q] = orQuery(docsRetrieved[q], myInvertedIndex.final_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTKiEEl_xc9f"
      },
      "source": [
        "def andQuery(list1, list2):\n",
        "  ans=0\n",
        "  p1=0\n",
        "  p2=0\n",
        "  while (p1<len(list1) and p2<len(list2)):\n",
        "    if (list1[p1] == int(list2[p2])):\n",
        "      ans=ans+1\n",
        "      p1=p1+1\n",
        "      p2=p2+1\n",
        "    else:\n",
        "      if list1[p1] > int(list2[p2]):\n",
        "        p2=p2+1\n",
        "      else:\n",
        "        p1=p1+1\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyUHVJifU7fd"
      },
      "source": [
        "relevant_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPuuCdFly4gX"
      },
      "source": [
        "PRECISION & RECALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoLPjLcOxnPp"
      },
      "source": [
        "precision=[]\n",
        "recall=[]\n",
        "for q in range(1,11):\n",
        "  relevantRetrieved=andQuery(docsRetrieved[q], relevant_docs[q-1])\n",
        "  precision.append(relevantRetrieved/len(docsRetrieved[q]))\n",
        "  recall.append(relevantRetrieved/len(relevant_docs[q-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ6FKXGi2nkH"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "queryID = [i+1 for i in range(len(precision))]\n",
        "plt.plot(queryID,precision,label = \"precision\")\n",
        "plt.plot(queryID,recall,label = \"recall\")\n",
        "plt.xlabel('QueryID') \n",
        "plt.ylabel('Performance scores') \n",
        "plt.title('Performance Graph')\n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nswna3pdFYV-"
      },
      "source": [
        "## **TASK 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af89ZLMl0h2O"
      },
      "source": [
        "LINGUISTIC MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT6a1LEK0g8O"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvWJH9Pj1q-G"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc5Sb5UU5to-"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUxozBor5yRL"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNaeo8Mu579q"
      },
      "source": [
        "spacy_processed_docs={}\n",
        "for d in documents.keys():\n",
        "  spacy_processed_docs[d] = (nlp(documents[d]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFFziRz7xx2"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "print(STOP_WORDS)\n",
        "print(type(STOP_WORDS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKEJiOf_A_lD"
      },
      "source": [
        "USING SPACY STOP WORDS TO IMPROVE PRECISION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWldQWIu720m"
      },
      "source": [
        "post_stop_word_removal={}\n",
        "for i in spacy_processed_docs.keys():\n",
        "  post_stop_word_removal[i] = [token.text for token in spacy_processed_docs[i] if not token.is_stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z_Xv9EY_EHM"
      },
      "source": [
        "spacyDocs={}\n",
        "seperator=' '\n",
        "for i in spacy_processed_docs.keys():\n",
        "  spacyDocs[i] = seperator.join(post_stop_word_removal[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_WUEWjg_x1C"
      },
      "source": [
        "myInvertedIndexP=getInvertedIndex(spacyDocs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3FWaMLRAJ2r"
      },
      "source": [
        "spacy_processed_query=[]\n",
        "for i in range(1,11):\n",
        "  temp=seperator.join(originalQueries[i])\n",
        "  spacy_processed_query.append(nlp(temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZlb_Aqri0KP"
      },
      "source": [
        "originalQueries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM7Leij7iU9T"
      },
      "source": [
        "spacy_processed_query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-9EJJiFBMCc"
      },
      "source": [
        "post_removal={}\n",
        "for i in range (10):\n",
        "  post_removal[i] = [token.text for token in spacy_processed_query[i] if not token.is_stop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYTiONkMibDJ"
      },
      "source": [
        "post_removal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFlMqQTtBQJw"
      },
      "source": [
        "RETRIEVING DOCS AFTER REMOVING SPACY STOP WORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r0IZbkJB2p9"
      },
      "source": [
        "spacy_docsRetrieved={}\n",
        "for q in range(10):\n",
        "  spacy_docsRetrieved[q]=[]\n",
        "  for word in post_removal[q]:\n",
        "    if word in myInvertedIndexP.final_dict:\n",
        "      spacy_docsRetrieved[q] = orQuery(spacy_docsRetrieved[q], myInvertedIndexP.final_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7RQjWbpBUYO"
      },
      "source": [
        "PRECISION AND RECALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkgAy8jeCI5x"
      },
      "source": [
        "spacy_precision=[]\n",
        "spacy_recall=[]\n",
        "for q in range(10):\n",
        "  relevantRetrieved=andQuery(spacy_docsRetrieved[q], relevant_docs[q])\n",
        "  spacy_precision.append(relevantRetrieved/len(spacy_docsRetrieved[q]))\n",
        "  spacy_recall.append(relevantRetrieved/len(relevant_docs[q]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRCMyxlHCooT"
      },
      "source": [
        "spacy_queryID = [i+1 for i in range(len(spacy_precision))]\n",
        "plt.plot(spacy_queryID,spacy_precision,label = \"precision\")\n",
        "plt.plot(spacy_queryID,spacy_recall,label = \"recall\")\n",
        "plt.xlabel('QueryID') \n",
        "plt.ylabel('Performance scores') \n",
        "plt.title('Performance Graph')\n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpJv5K-8BYrA"
      },
      "source": [
        "USING LANCASTER STEMMER TO IMPROVE RECALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXYMVfRqe_Ts"
      },
      "source": [
        "from nltk.stem.snowball import PorterStemmer,SnowballStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgguBV4kfNrk"
      },
      "source": [
        "ls = LancasterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB3VzQZYfWNs"
      },
      "source": [
        "stemmer_processed_docs={}\n",
        "for d in documents.keys():\n",
        "  li = list(documents[d].split(\" \")) \n",
        "  for i in range(len(li)):\n",
        "    li[i]=ls.stem(li[i])\n",
        "  stemmer_processed_docs[d] = seperator.join(li)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77kOHWOPfgDy"
      },
      "source": [
        "myInvertedIndexR=getInvertedIndex(stemmer_processed_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVrbgG2PfjWD"
      },
      "source": [
        "stemmer_processed_query=[]\n",
        "for i in range(1,11):\n",
        "  li=[]\n",
        "  for j in range(len(originalQueries[i])):\n",
        "    li.append(ls.stem(originalQueries[i][j]))\n",
        "  stemmer_processed_query.append(seperator.join(li))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF2DZ5mQBjZo"
      },
      "source": [
        "RETRIEVING DOCS AFTER USING LANCASTER STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3Xie7mNjhpG"
      },
      "source": [
        "stemmer_docsRetrieved={}\n",
        "for q in range(10):\n",
        "  stemmer_docsRetrieved[q]=[]\n",
        "  for word in (stemmer_processed_query[q].split(\" \")):\n",
        "    if word in myInvertedIndexR.final_dict:\n",
        "      stemmer_docsRetrieved[q] = orQuery(stemmer_docsRetrieved[q], myInvertedIndexR.final_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3SZJ5VoBsLm"
      },
      "source": [
        "PRECISION AND RECALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1Ngxw58lENA"
      },
      "source": [
        "stemmer_precision=[]\n",
        "stemmer_recall=[]\n",
        "for q in range(10):\n",
        "  relevantRetrieved=andQuery(stemmer_docsRetrieved[q], relevant_docs[q])\n",
        "  stemmer_precision.append(relevantRetrieved/len(stemmer_docsRetrieved[q]))\n",
        "  stemmer_recall.append(relevantRetrieved/len(relevant_docs[q]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG_O6VuklYfD"
      },
      "source": [
        "stemmer_queryID = [i+1 for i in range(len(stemmer_precision))]\n",
        "plt.plot(stemmer_queryID,stemmer_precision,label = \"precision\")\n",
        "plt.plot(stemmer_queryID,stemmer_recall,label = \"recall\")\n",
        "plt.xlabel('QueryID') \n",
        "plt.ylabel('Performance scores') \n",
        "plt.title('Performance Graph')\n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjs6jUBkFk1V"
      },
      "source": [
        "##**TASK 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yfYkWIfFsOA"
      },
      "source": [
        "***Task3 A:***\n",
        "Changes made in the dataset for improving on precision:\n",
        "Removal of stop-words using spacy:\n",
        "We observed that there were instances of documents having a lot of stop words like “as”, “in”  etc. Owing to these, a lot of non-relevant documents were getting retrieved.\n",
        "\t\n",
        "Changes made in the dataset for improving on recall:\n",
        "We did try to remove stop words, using spacy, but, unexpectedly this led to a decrease in recall. \n",
        "*For Ex*. One document had a lot of stop words like “is”, “an” etc. but also had a relevant word *“mechanisms”*. Even though my query had *“mechanism”* but the reason it was initially retrieved was because of overlapping stop words. Now that all the stop words were removed and no other linguistic model was used, this document did not get retrieved.\n",
        "Use of stemming:\n",
        "We decided to go with Porter’s stemmer but we observed that only the following conversions were being made equivalent: \n",
        "Controlled --- control\n",
        "We tried LancasterStemmer after that and in addition to the making the above two words equivalent, it was also making the following conversions:\n",
        "Amplifiers --- amplification\n",
        "\n",
        "***Task3 B:***\n",
        "Changes in Recall for respective queries:\n",
        "Before using stemming :\n",
        "[1.00, 1.00, 1.00, 1.00, 1.00, 0.96, 0.91, 0.87, 1.00, 1.00]\n",
        "After using stemming:\n",
        "[1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.95, 0.87, 1.00, 1.00]                              \n",
        "\n",
        " Changes in Precision for respective queries:\n",
        " Before stopword removal:\n",
        "[0.15, 0.17, 0.07, 0.02, 0.13, 0.30, 0.13, 0.11, 0.24, 0.04]\n",
        "After stopword removal:\n",
        "[0.36, 0.36, 0.50, 0.12, 0.37, 0.30, 0.35, 0.12, 0.24, 0.22]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le2znQmQEGNn"
      },
      "source": [
        "## **TASK 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z95Uodjb2E3A"
      },
      "source": [
        "def and_query(list1, list2):\n",
        "  ans=[]\n",
        "  p1=0\n",
        "  p2=0\n",
        "  while (p1<len(list1) and p2<len(list2)):\n",
        "    if (list1[p1] == list2[p2]):\n",
        "      ans.append(list1[p1])\n",
        "      p1=p1+1\n",
        "      p2=p2+1\n",
        "    else:\n",
        "      if list1[p1] > list2[p2]:\n",
        "        p2=p2+1\n",
        "      else:\n",
        "        p1=p1+1\n",
        "  return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq1K73W32UIG"
      },
      "source": [
        "bi_gram_dict={}\n",
        "for word in myInvertedIndex.final_dict.keys():\n",
        "  temp = '$' + word + '$'\n",
        "  for i in range(len(temp)-1):\n",
        "    bi = temp[i]+temp[i+1]\n",
        "    if (bi in bi_gram_dict.keys()):\n",
        "      if (word not in bi_gram_dict[bi]):\n",
        "        bi_gram_dict[bi].append(word)\n",
        "    else:\n",
        "      bi_gram_dict[bi] = [word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOw0lsvu2XhW"
      },
      "source": [
        "for bi in bi_gram_dict.keys():\n",
        "  bi_gram_dict[bi].sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPUZtxzQCQ15"
      },
      "source": [
        "for q in queries.keys():\n",
        "  queries[q][2] = queries[q][2][:2] + '*' + queries[q][2][2:]\n",
        "  queries[q][4] = '*' + queries[q][4]\n",
        "  queries[q][1] = queries[q][1] + '*'\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be8e-tph2amc"
      },
      "source": [
        "doc_ret = {}\n",
        "for q in queries.keys():\n",
        "  doc_ret[q] = []\n",
        "  for word in queries[q]:\n",
        "    if ('*' in word):\n",
        "      temp = '$'+word+'$'\n",
        "      \n",
        "      word_list = []\n",
        "      f=0\n",
        "      for i in range(len(temp)-1):\n",
        "        if (temp[i]!='*' and temp[i+1]!='*'):\n",
        "          bi = temp[i]+temp[i+1]\n",
        "          if (f==0):\n",
        "            word_list = bi_gram_dict[bi]\n",
        "            f=1\n",
        "          else:\n",
        "            word_list = and_query(word_list, bi_gram_dict[bi])\n",
        "      for w in word_list:\n",
        "        if (w in myInvertedIndex.final_dict.keys()):\n",
        "          doc_ret[q] = orQuery(doc_ret[q], myInvertedIndex.final_dict[w])\n",
        "\n",
        "    else:\n",
        "      if (word in myInvertedIndex.final_dict.keys()):\n",
        "        doc_ret[q] = orQuery(doc_ret[q], myInvertedIndex.final_dict[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O3LT12aEVM-"
      },
      "source": [
        "PRECISION AND RECALL WITHOUT ANY LINGUISTIC MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HLdImSG2drQ"
      },
      "source": [
        "bi_precision=[]\n",
        "bi_recall=[]\n",
        "for q in range(1,11):\n",
        "  relevantRetrieved=andQuery(doc_ret[q], relevant_docs[q-1])\n",
        "  bi_precision.append(relevantRetrieved/len(doc_ret[q]))\n",
        "  bi_recall.append(relevantRetrieved/len(relevant_docs[q-1]))\n",
        "\n",
        "print(bi_precision)\n",
        "print(bi_recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmzRy5zzCYNJ"
      },
      "source": [
        "plt.plot(queryID,bi_precision,label = \"precision\")\n",
        "plt.plot(queryID,bi_recall,label = \"recall\")\n",
        "plt.xlabel('QueryID') \n",
        "plt.ylabel('Performance scores') \n",
        "plt.title('Performance Graph')\n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA-xk3yw2zKf"
      },
      "source": [
        "bi_gram_dict_P={}\n",
        "for word in myInvertedIndexP.final_dict.keys():\n",
        "  temp = '$' + word + '$'\n",
        "  for i in range(len(temp)-1):\n",
        "    bi = temp[i]+temp[i+1]\n",
        "    if (bi in bi_gram_dict_P.keys()):\n",
        "      if (word not in bi_gram_dict_P[bi]):\n",
        "        bi_gram_dict_P[bi].append(word)\n",
        "    else:\n",
        "      bi_gram_dict_P[bi] = [word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV8103h63Q0S"
      },
      "source": [
        "for bi in bi_gram_dict_P.keys():\n",
        "  bi_gram_dict_P[bi].sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68AoBfDVCcBv"
      },
      "source": [
        "for q in post_removal.keys():\n",
        "  post_removal[q][2] = post_removal[q][2][:2] + '*' + post_removal[q][2][2:]\n",
        "  post_removal[q][4] = '*' + post_removal[q][4]\n",
        "  post_removal[q][1] = post_removal[q][1] + '*'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd_LDl1T3YiO"
      },
      "source": [
        "doc_ret_P = {}\n",
        "for q in post_removal.keys():\n",
        "  doc_ret_P[q] = []\n",
        "  for word in post_removal[q]:\n",
        "    if ('*' in word):\n",
        "      temp = '$'+word+'$'\n",
        "      \n",
        "      word_list = []\n",
        "      f=0\n",
        "      for i in range(len(temp)-1):\n",
        "        if (temp[i]!='*' and temp[i+1]!='*'):\n",
        "          bi = temp[i]+temp[i+1]\n",
        "          if (f==0):\n",
        "            word_list = bi_gram_dict_P[bi]\n",
        "            f=1\n",
        "          else:\n",
        "            word_list = and_query(word_list, bi_gram_dict_P[bi])\n",
        "  \n",
        "      for w in word_list:\n",
        "        if (w in myInvertedIndexP.final_dict.keys()):\n",
        "          doc_ret_P[q] = orQuery(doc_ret_P[q], myInvertedIndexP.final_dict[w])\n",
        "\n",
        "    else:\n",
        "      if (word in myInvertedIndexP.final_dict.keys()):\n",
        "        doc_ret_P[q] = orQuery(doc_ret_P[q], myInvertedIndexP.final_dict[word])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgeTCuxCEm-q"
      },
      "source": [
        "PRECISION AND RECALL USING SPACY STOPWORDS REMOVAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag62v8So33My"
      },
      "source": [
        "bi_precision_P=[]\n",
        "bi_recall_P=[]\n",
        "for q in range(10):\n",
        "  relevantRetrieved=andQuery(doc_ret_P[q], relevant_docs[q])\n",
        "  bi_precision_P.append(relevantRetrieved/len(doc_ret_P[q]))\n",
        "  bi_recall_P.append(relevantRetrieved/len(relevant_docs[q]))\n",
        "\n",
        "print(bi_precision_P)\n",
        "print(bi_recall_P)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvciheiYCdZ9"
      },
      "source": [
        "plt.plot(queryID,bi_precision_P,label = \"precision\")\n",
        "plt.plot(queryID,bi_recall_P,label = \"recall\")\n",
        "plt.xlabel('QueryID') \n",
        "plt.ylabel('Performance scores') \n",
        "plt.title('Performance Graph')\n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97ZkJ4n3Cr-i"
      },
      "source": [
        "stemmer_query = {}\n",
        "for i in range(10):\n",
        "  stemmer_query[i] = stemmer_processed_query[i].split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0WxasrQCsmH"
      },
      "source": [
        "for q in stemmer_query.keys():\n",
        "  stemmer_query[q][2] = stemmer_query[q][2][:2] + '*' + stemmer_query[q][2][2:]\n",
        "  stemmer_query[q][4] = '*' + stemmer_query[q][4]\n",
        "  stemmer_query[q][1] = stemmer_query[q][1] + '*'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAMBPFwV4DH-"
      },
      "source": [
        "bi_gram_dict_R={}\n",
        "for word in myInvertedIndexR.final_dict.keys():\n",
        "  temp = '$' + word + '$'\n",
        "  for i in range(len(temp)-1):\n",
        "    bi = temp[i]+temp[i+1]\n",
        "    if (bi in bi_gram_dict_R.keys()):\n",
        "      if (word not in bi_gram_dict_R[bi]):\n",
        "        bi_gram_dict_R[bi].append(word)\n",
        "    else:\n",
        "      bi_gram_dict_R[bi] = [word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H8NlpWn4V_e"
      },
      "source": [
        "for bi in bi_gram_dict_R.keys():\n",
        "  bi_gram_dict_R[bi].sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydwd_SGH4bPQ"
      },
      "source": [
        "doc_ret_R = {}\n",
        "for q in stemmer_query.keys():\n",
        "  doc_ret_R[q] = []\n",
        "  for word in stemmer_query[q]:\n",
        "    if ('*' in word):\n",
        "      temp = '$'+word+'$'\n",
        "      \n",
        "      word_list = []\n",
        "      f=0\n",
        "      for i in range(len(temp)-1):\n",
        "        if (temp[i]!='*' and temp[i+1]!='*'):\n",
        "          bi = temp[i]+temp[i+1]\n",
        "          if (f==0):\n",
        "            if (bi in bi_gram_dict_R.keys()):\n",
        "              word_list = bi_gram_dict_R[bi]\n",
        "              f=1\n",
        "          else:\n",
        "            if (bi in bi_gram_dict_R.keys()):\n",
        "              word_list = and_query(word_list, bi_gram_dict_R[bi])\n",
        "      if (q==1):\n",
        "        print(temp,word_list)\n",
        "      for w in word_list:\n",
        "        if (w in myInvertedIndexR.final_dict.keys()):\n",
        "          doc_ret_R[q] = orQuery(doc_ret_R[q], myInvertedIndexR.final_dict[w])\n",
        "\n",
        "    else:\n",
        "      if (word in myInvertedIndexR.final_dict.keys()):\n",
        "        doc_ret_R[q] = orQuery(doc_ret_R[q], myInvertedIndexR.final_dict[word])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDG9jFUSE8Ev"
      },
      "source": [
        "PRECISION AND REMOVAL USING LANCASTER STEMMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PdcdjhC4zyz"
      },
      "source": [
        "bi_precision_R=[]\n",
        "bi_recall_R=[]\n",
        "for q in range(10):\n",
        "  relevantRetrieved=andQuery(doc_ret_R[q], relevant_docs[q])\n",
        "  bi_precision_R.append(relevantRetrieved/len(doc_ret_R[q]))\n",
        "  bi_recall_R.append(relevantRetrieved/len(relevant_docs[q]))\n",
        "\n",
        "print(bi_precision_R)\n",
        "print(bi_recall_R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi2_oU-OCyBO"
      },
      "source": [
        "plt.plot(queryID,bi_precision_R,label = \"precision\")\n",
        "plt.plot(queryID,bi_recall_R,label = \"recall\")\n",
        "plt.xlabel('QueryID') \n",
        "plt.ylabel('Performance scores') \n",
        "plt.title('Performance Graph')\n",
        "plt.legend() \n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTSMleC5Gp3j"
      },
      "source": [
        "JUSTIFICATION :- Different wild card queries were made for all the different models and processed with the help of bi-gram indexing. The results of Precision and Recall of different models were consistent with the previous results described above. \n",
        "\n",
        "The precision of the full model with wildcard queries was increased with the help of using spacy stop word removal on the wildcard queries, while the recall was improved by employing Lancaster stemmer for wildcard queries.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEasLEwofSTV"
      },
      "source": [
        "ASSIGNMENT-2 starts from here!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xulzw2rkAff"
      },
      "source": [
        "all variables used in the second assignment must end with 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw30xSTNfXT7"
      },
      "source": [
        "myInvertedIndex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzcYFQXbfhib"
      },
      "source": [
        "originalQueries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0yEb5gdl9B-"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLi5dSpKlkP-"
      },
      "source": [
        "def removeStopWords(tokenizedSent):\n",
        "  '''\n",
        "  :params:\n",
        "  tokenizedSent: list of individual tokens\n",
        "\n",
        "  :returns:\n",
        "  list of individual tokens w/o the stop words\n",
        "  '''\n",
        "  sw = set(stopwords.words('english'))\n",
        "  return [w for w in tokenizedSent if w not in sw]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZz5vXs5je6g"
      },
      "source": [
        "post_stop_removal2={}\n",
        "for i in range (1,11):\n",
        "  post_stop_removal2[i] = removeStopWords(originalQueries[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-JJmnDVj4vD"
      },
      "source": [
        "post_stop_removal2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiDhbdEBj6RV"
      },
      "source": [
        "for i in range(1,11):\n",
        "  for s in post_stop_removal2[i]:\n",
        "    s=s.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRApfDNpkgSd"
      },
      "source": [
        "post_stop_removal2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJUaa8t5oTz2"
      },
      "source": [
        "type(documents[129])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltIIyW4zozw3"
      },
      "source": [
        "## HELPER FUNCTIONS\n",
        "\n",
        "def lowerSent(sentence):\n",
        "  '''\n",
        "  :params:\n",
        "  sentence: complete input string\n",
        "\n",
        "  :returns:\n",
        "  lower-cased input string\n",
        "  '''\n",
        "  return sentence.lower()\n",
        "\n",
        "def tokenizeSent(sentence):\n",
        "  '''\n",
        "  :params:\n",
        "  sentence: complete input string\n",
        "\n",
        "  :returns:\n",
        "  list of individual tokens\n",
        "  '''\n",
        "  tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")\n",
        "  return tokenizer.tokenize(sentence)\n",
        "\n",
        "def removeStopWords(tokenizedSent):\n",
        "  '''\n",
        "  :params:\n",
        "  tokenizedSent: list of individual tokens\n",
        "\n",
        "  :returns:\n",
        "  list of individual tokens w/o the stop words\n",
        "  '''\n",
        "  sw = set(stopwords.words('english'))\n",
        "  return [w for w in tokenizedSent if w not in sw]\n",
        "  \n",
        "def stem(tokenizedSent):\n",
        "  '''\n",
        "  :params:\n",
        "  tokenizedSent: list of individual tokens\n",
        "\n",
        "  :returns:\n",
        "  list of stemmed individual tokens\n",
        "  '''\n",
        "  stemmer = PorterStemmer()\n",
        "  return [stemmer.stem(w) for w in tokenizedSent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4gYMIs4o33A"
      },
      "source": [
        "def myTokenizer(sentence):\n",
        "  '''\n",
        "  :params:\n",
        "  sentence: complete input string\n",
        "\n",
        "  :returns:\n",
        "  list of preprocessed individual tokens\n",
        "  '''\n",
        "  loweredSent = lowerSent(sentence)\n",
        "  tokenizedSent = tokenizeSent(loweredSent)\n",
        "  tokenizedSent = removeStopWords(tokenizedSent)\n",
        "  stemmedSent = stem(tokenizedSent)\n",
        "  preprocessedSent = stemmedSent\n",
        "\n",
        "  return preprocessedSent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXbEJbMgq71V"
      },
      "source": [
        "def getTokens(sentence):\n",
        "  tokens=[]\n",
        "  t1 = sentence.split() \n",
        "  t2 = t1[1:]\n",
        "\n",
        "  return t2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LTXz7Y0sk0M"
      },
      "source": [
        "from collections import defaultdict,Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7t4amy-2kh2Z"
      },
      "source": [
        "def get_tf_doc(doc_id):\n",
        "    \"\"\"\n",
        "    :params:\n",
        "    docId: Id of a document\n",
        "\n",
        "    :returns:\n",
        "    a dictionary correpsonding to token:frequency(token) of tokens present in the document represented by docId \n",
        "  \n",
        "\"\"\"\n",
        "    tokens=getTokens(documents[doc_id])\n",
        "    tf_dict = Counter(tokens)\n",
        "    return tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBfAHvwDo_dM"
      },
      "source": [
        "get_tf_doc(129)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYRxe0Wv5xrf"
      },
      "source": [
        "doc_ids=[]\n",
        "for ids in documents.keys():\n",
        "  doc_ids.append(ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Tw6JkJLseMV"
      },
      "source": [
        "docs_tf_dict = {}\n",
        "for docId in doc_ids:\n",
        "    docs_tf_dict[docId] = get_tf_doc(docId)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx5GALEOs-Bm"
      },
      "source": [
        "docs_tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG34Mnh7unH9"
      },
      "source": [
        "def read_doc(docId):\n",
        "    \"\"\"\n",
        "    Utility function to read the contenst of a document given its docId\n",
        "    \"\"\"\n",
        "    with open(os.path.join(os.getcwd(),\"Working_Sample\",docId),'r') as f:\n",
        "        doc_text=f.read()\n",
        "    return doc_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lq-InzEtFsW"
      },
      "source": [
        "def get_df(doc_ids):\n",
        "    '''\n",
        "    :params:\n",
        "    doc_ids: set of documents [corpus]\n",
        "\n",
        "    :returns:\n",
        "     a dictionary corresponding to token:Document Frequency(token) i.e the number of documents that token is present in\n",
        "    '''\n",
        "\n",
        "    result_dict = None\n",
        "    for doc in doc_ids:\n",
        "        doc_tokens = getTokens(documents[doc])\n",
        "        temp_df_dict = Counter(set(doc_tokens))\n",
        "        if result_dict:\n",
        "            result_dict = result_dict + temp_df_dict\n",
        "        else:\n",
        "            result_dict = temp_df_dict\n",
        "    return result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxX2XngyuVqE"
      },
      "source": [
        "terms_df_dict=get_df(doc_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnCDjJZIwkaO"
      },
      "source": [
        "from math import log10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0pAR-hBufzd"
      },
      "source": [
        "idf_dict = {}\n",
        "for key,val in terms_df_dict.items():\n",
        "    idf_dict[key] = log10(len(documents)/float(val)) \n",
        "    ## AT TIMES N/(1+ df) IS USED TO HANDLE THE CASE WHERE DF = 0, HOWEVER, THAT CAN LEAD TO IDF BEING NEGATIVE\n",
        "print(\"The idf dictionary is:\")\n",
        "idf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aE2KoQ1nwgBK"
      },
      "source": [
        "def TFIDFRetrieval(query, documents, docs_tf_dict=None, idf_dict=None):\n",
        "  '''\n",
        "  :params:\n",
        "  query: user input [query string]\n",
        "  documents: set of documents [corpus]\n",
        "  docs_tf_dict: dictionary of term-frequencies for each document [OPTIONAL]\n",
        "  idf_dict: dictionary of inverse document frequencies for each term in corpus [OPTIONAL]\n",
        "\n",
        "  :returns:\n",
        "  list of sorted [decreasing] tuples where each tuple comprises of the document ID and the associated score\n",
        "  '''\n",
        "  if docs_tf_dict == None:\n",
        "    docs_tf_dict = {}\n",
        "    for docId in documents:\n",
        "      docs_tf_dict[docId] = get_tf_doc(docId)\n",
        "  \n",
        "  if idf_dict == None:\n",
        "    df_dict = get_df(documents)\n",
        "    idf_dict = {}\n",
        "    for key,val in df_dict.items():\n",
        "      idf_dict[key] = log10(len(spacyDocs)/float(val))\n",
        "\n",
        "  query_tokens = query\n",
        "  results = []\n",
        "  for doc in documents:\n",
        "    score = 0\n",
        "    for token in query_tokens:\n",
        "      try:\n",
        "        score += (log10(docs_tf_dict[doc][token]+1) * idf_dict[token])\n",
        "      except:\n",
        "        score += 0\n",
        "    results.append([score,doc])\n",
        "  return sorted(results, reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubAsi7arxRyX"
      },
      "source": [
        "TFIDFRetrieval(post_stop_removal2[1],doc_ids, docs_tf_dict=docs_tf_dict, idf_dict=idf_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-MXCHulxlEB"
      },
      "source": [
        "print(\"Query IDs | \\tDoc IDs \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t| \\tScores \")\n",
        "for qid,q in originalQueries.items():\n",
        "  results=TFIDFRetrieval(q,documents, docs_tf_dict=docs_tf_dict, idf_dict=idf_dict)\n",
        "  print(qid, \"\\t  | \",end=\"\\t\")\n",
        "  for i in range (20):\n",
        "    print(\"{0:d} \".format(results[i][1]),end=\"\\t\")\n",
        "  print(\" \\t| \", end=\"\\t\")\n",
        "  for i in range (20):\n",
        "    print(\"{0:2.2f} \" .format(results[i][0]),end=\"\\t\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4HKm3urTJxI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk4BXm72TaS3"
      },
      "source": [
        "Top K precision without any modification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knPSDOe-Tfo0"
      },
      "source": [
        "r[9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgkZAFerVtYO"
      },
      "source": [
        "rel_docs_dict={}\n",
        "for str in r:\n",
        "  t1=str.split(' ');\n",
        "  t2=t1[1:]\n",
        "  q_no=int(t1[0])\n",
        "  for s in t2:\n",
        "    if s=='':\n",
        "      continue\n",
        "    s=int(s)\n",
        "    if s not in rel_docs_dict:\n",
        "      rel_docs_dict[s]=[]\n",
        "    rel_docs_dict[s].append(q_no)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWNFemNwW6cY"
      },
      "source": [
        "rel_docs_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4h9qeH8YidP"
      },
      "source": [
        "precision_k={}\n",
        "total=0\n",
        "rel=0\n",
        "for qid,q in post_stop_removal2.items():\n",
        "  total=0\n",
        "  rel=0\n",
        "  results=TFIDFRetrieval(q,documents, docs_tf_dict=docs_tf_dict, idf_dict=idf_dict)\n",
        "  print(qid, \"\\t  | \",end=\"\\t\")\n",
        "  precision_k[qid]=[]\n",
        "  for i in range (20):\n",
        "    total+=1\n",
        "    if results[i][1] in rel_docs_dict:\n",
        "      for s in rel_docs_dict[results[i][1]]:\n",
        "        if(s==qid):\n",
        "          rel+=1\n",
        "          break\n",
        "    precision_k[qid].append(rel/total)\n",
        "    print(\"{0:d} \".format(results[i][1]),end=\"\\t\")\n",
        "  print(\" \\t| \", end=\"\\t\")\n",
        "  for i in range (20):\n",
        "    print(\"{0:2.2f} \" .format(results[i][0]),end=\"\\t\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wNtcedPZpKp"
      },
      "source": [
        "precision_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOhOahpwVttR"
      },
      "source": [
        "**Increase** **Precision**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp55464Q0sxc"
      },
      "source": [
        "spacyDocs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VBCMlVVQkOk"
      },
      "source": [
        "def get_tf_doc_spacy(doc_id):\n",
        "    \"\"\"\n",
        "    :params:\n",
        "    docId: Id of a document\n",
        "\n",
        "    :returns:\n",
        "    a dictionary correpsonding to token:frequency(token) of tokens present in the document represented by docId \n",
        "  \n",
        "\"\"\"\n",
        "\n",
        "    tokens=myTokenizer(spacyDocs[doc_id])\n",
        "    print(tokens)\n",
        "    tf_dict = Counter(tokens)\n",
        "    return tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT3iePgmPvD6"
      },
      "source": [
        "spacy_tf_dict={}\n",
        "for docId in doc_ids:\n",
        "    spacy_tf_dict[docId] = get_tf_doc_spacy(docId)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48YcEyMIQ7Vi"
      },
      "source": [
        "spacy_tf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0zejq4nRTxY"
      },
      "source": [
        "def get_df_spacy(doc_ids):\n",
        "    '''\n",
        "    :params:\n",
        "    doc_ids: set of documents [corpus]\n",
        "\n",
        "    :returns:\n",
        "     a dictionary corresponding to token:Document Frequency(token) i.e the number of documents that token is present in\n",
        "    '''\n",
        "\n",
        "    result_dict = None\n",
        "    for doc in doc_ids:\n",
        "        doc_tokens = myTokenizer(spacyDocs[doc])\n",
        "        temp_df_dict = Counter(set(doc_tokens))\n",
        "        if result_dict:\n",
        "            result_dict = result_dict + temp_df_dict\n",
        "        else:\n",
        "            result_dict = temp_df_dict\n",
        "    return result_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxAabYO3Q9UX"
      },
      "source": [
        "import copy\n",
        "spacy_df_dict = copy.deepcopy(get_df_spacy(doc_ids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYhpBQAHRfOv"
      },
      "source": [
        "spacy_df_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5c44XRdwRhPJ"
      },
      "source": [
        "spacy_idf_dict = {}\n",
        "for key,val in spacy_df_dict.items():\n",
        "    spacy_idf_dict[key] = log10(len(spacyDocs)/float(val)) \n",
        "    ## AT TIMES N/(1+ df) IS USED TO HANDLE THE CASE WHERE DF = 0, HOWEVER, THAT CAN LEAD TO IDF BEING NEGATIVE\n",
        "print(\"The idf dictionary is:\")\n",
        "spacy_idf_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWHaPDqcRvWL"
      },
      "source": [
        "TFIDFRetrieval(post_stop_removal2[1], doc_ids, docs_tf_dict=spacy_tf_dict, idf_dict=spacy_idf_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6up858kStLm"
      },
      "source": [
        "print(\"Query IDs | \\tDoc IDs \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t| \\tScores \")\n",
        "for qid,q in post_stop_removal2.items():\n",
        "  results=TFIDFRetrieval(q,spacyDocs, docs_tf_dict=spacy_tf_dict, idf_dict=spacy_idf_dict)\n",
        "  print(qid, \"\\t  | \",end=\"\\t\")\n",
        "  for i in range (20):\n",
        "    print(\"{0:d} \".format(results[i][1]),end=\"\\t\")\n",
        "  print(\" \\t| \", end=\"\\t\")\n",
        "  for i in range (20):\n",
        "    print(\"{0:2.2f} \" .format(results[i][0]),end=\"\\t\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhIaEp3FVjIs"
      },
      "source": [
        "spacy_precision_k={}\n",
        "total=0\n",
        "rel=0\n",
        "for qid,q in post_stop_removal2.items():\n",
        "  total=0\n",
        "  rel=0\n",
        "  results=TFIDFRetrieval(q,spacyDocs, docs_tf_dict=spacy_tf_dict, idf_dict=spacy_idf_dict)\n",
        "  print(qid, \"\\t  | \",end=\"\\t\")\n",
        "  spacy_precision_k[qid]=[]\n",
        "  for i in range (20):\n",
        "    total+=1\n",
        "    if results[i][1] in rel_docs_dict:\n",
        "      for s in rel_docs_dict[results[i][1]]:\n",
        "        if(s==qid):\n",
        "          rel+=1\n",
        "          break\n",
        "    spacy_precision_k[qid].append(rel/total)\n",
        "    print(\"{0:d} \".format(results[i][1]),end=\"\\t\")\n",
        "  print(\" \\t| \", end=\"\\t\")\n",
        "  for i in range (20):\n",
        "    print(\"{0:2.2f} \" .format(results[i][0]),end=\"\\t\")\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnV1lfd3ct2t"
      },
      "source": [
        "spacy_precision_k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK26FbiKcxOG"
      },
      "source": [
        "post_stop_removal2[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17HPhE1ddkii"
      },
      "source": [
        "print(originalQueries)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPektmO4wB2o"
      },
      "source": [
        "## Task2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D1t3cvfdv57"
      },
      "source": [
        "import copy\n",
        "spell_error_queries = copy.deepcopy(post_stop_removal2)\n",
        "print(spell_error_queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jah1_uDxxlCQ"
      },
      "source": [
        "import random\n",
        "import string\n",
        "for query_id in spell_error_queries.keys():\n",
        "  temp = []\n",
        "  for i in range(len(spell_error_queries[query_id] )):\n",
        "    if len(spell_error_queries[query_id][i])>2:\n",
        "      temp.append(i)\n",
        "  r1 = random.choice(temp)\n",
        "  w1 = spell_error_queries[query_id][r1]\n",
        "  delchar = random.randint(1,len(w1)-1)\n",
        "  tw1 = w1[:delchar-1]+ w1[delchar:]\n",
        "  spell_error_queries[query_id][r1] = tw1\n",
        "  temp.remove(r1)\n",
        "\n",
        "  r2 = random.choice(temp)\n",
        "  w2 = spell_error_queries[query_id][r2]\n",
        "  delchar = random.randint(1,len(w2)-1)\n",
        "  tw2 = w2[:delchar-1]+ w2[delchar:]\n",
        "  r22 = random.randint(0,len(tw2)-1)\n",
        "  tw22 = tw2[:r22] + random.choice(string.ascii_lowercase) + tw2[r22:]\n",
        "  spell_error_queries[query_id][r2] = tw22\n",
        "  temp.remove(r2)\n",
        "\n",
        "  r3 = random.choice(temp)\n",
        "  w3 = spell_error_queries[query_id][r3]\n",
        "  l = len(w3)\n",
        "  pos1 = random.randint(0,l//2-1)\n",
        "  pos2 = random.randint(l//2+1,l-1)\n",
        "  w33 = [char for char in  w3]\n",
        "  c = w33[pos1]\n",
        "  w33[pos1]= w33[pos2]\n",
        "  w33[pos2] = c\n",
        "  fw3 = ''.join(w33)\n",
        "  spell_error_queries[query_id][r3] = fw3\n",
        "  print(w1,w2,w3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9470m_g5q-0"
      },
      "source": [
        "def editDistDP(str1, str2):\n",
        "    m = len(str1);\n",
        "    n = len(str2); \n",
        "    dp = [[0 for x in range(n + 1)] for x in range(m + 1)] \n",
        "    for i in range(m + 1): \n",
        "        for j in range(n + 1): \n",
        "            if i == 0: \n",
        "                dp[i][j] = j \n",
        "            elif j == 0: \n",
        "                dp[i][j] = i \n",
        "            elif str1[i-1] == str2[j-1]: \n",
        "                dp[i][j] = dp[i-1][j-1] \n",
        "            else: \n",
        "                dp[i][j] = min(1+dp[i][j-1], 1+dp[i-1][j], 2+dp[i-1][j-1])\n",
        "  \n",
        "    return dp[m][n] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PyL5ar1CoIT"
      },
      "source": [
        "myInvertedIndex2 = getInvertedIndex(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paDbCCU0C9tE"
      },
      "source": [
        "corrected_queries = {}\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Query IDs', 'Typo', 'Term', 'Distance'])\n",
        "for i in spell_error_queries.keys():\n",
        "  corrected_queries[i] = []\n",
        "  for word in spell_error_queries[i]:\n",
        "    mini = 5\n",
        "    corr_word = word\n",
        "    listRow = [];\n",
        "    listRow.append(i);\n",
        "    listRow.append(word);\n",
        "    for original_word in myInvertedIndex2.final_dict.keys():\n",
        "      if original_word in STOP_WORDS:\n",
        "        continue\n",
        "      if (editDistDP(word, original_word)<mini):\n",
        "        mini = editDistDP(word, original_word)\n",
        "        corr_word = original_word\n",
        "    listRow.append(corr_word);\n",
        "    listRow.append(mini);\n",
        "    corrected_queries[i].append(corr_word)\n",
        "    if corr_word != word:\n",
        "      t.add_row(listRow);\n",
        "print(t);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGt51aUexnyU"
      },
      "source": [
        "print(spell_error_queries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IlkWkNCFpuH"
      },
      "source": [
        "print(corrected_queries[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T47wAf2SLKcT"
      },
      "source": [
        "precision_k_spell={}\n",
        "total=0\n",
        "rel=0\n",
        "for qid,q in corrected_queries.items():\n",
        "  total=0\n",
        "  rel=0\n",
        "  results=TFIDFRetrieval(q,documents)\n",
        "  precision_k_spell[qid]=[]\n",
        "  for i in range(5):\n",
        "    total+=1\n",
        "    if results[i][1] in rel_docs_dict:\n",
        "      for s in rel_docs_dict[results[i][1]]:\n",
        "        if(s==qid):\n",
        "          rel+=1\n",
        "          break\n",
        "    precision_k_spell[qid].append(rel/total)\n",
        "    print(\"{0:d} \".format(results[i][1]),end=\"\\t\")\n",
        "  for i in range (5):\n",
        "    print(\"{0:2.2f} \" .format(results[i][0]),end=\"\\t\")\n",
        "  print()\n",
        "precision_k_spell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzFC8Ii0QEYw"
      },
      "source": [
        "#fingerkeying - an implementation of QWERTY Keyboard\n",
        "#Author Ekta Grover ekta1007@gmail.com on 19th July, 2013\n",
        "\n",
        "from __future__ import division\n",
        "from itertools import chain\n",
        "#import pydot\n",
        "from random import random\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "G=nx.Graph()\n",
        "# Keying in the structure of the QWERTY keyboard\n",
        "row1=['q','w','e','r','t','y','u','i','o','p']\n",
        "row2=['a','s','d','f','g','h','j','k','l']\n",
        "row3=['z','x','c','v','b','n','m']\n",
        "\n",
        "# Dividing the keyboard in left and right \n",
        "\n",
        "right_keys=list(chain.from_iterable([row1[0:5],row2[0:5],row3[0:5]]))\n",
        "left_keys=list(chain.from_iterable([row1[5:10],row2[5:9],row3[5:7]]))\n",
        "\n",
        "# add each rows independetly with the corresponding weights - we will use only weight of 1 , so we connect only adjacent nodes\n",
        "for i in range(0,len(row1)-1):\n",
        "    G.add_edge(row1[i],row1[i+1],weight=1,color='red')\n",
        "  \n",
        "for i in range(0,len(row2)-1):\n",
        "    G.add_edge(row2[i],row2[i+1],weight=1,color='blue')\n",
        "    \n",
        "for i in range(0,len(row3)-1):\n",
        "    G.add_edge(row3[i],row3[i+1],weight=1,color='green')\n",
        "\n",
        "# add the next row in there - row1 with row2\n",
        "for i in range(0, len(row1)):\n",
        "    if i==0:\n",
        "        G.add_edge(row1[i],row2[i],weight=1,color='cyan')\n",
        "        G.add_edge(row1[i],row2[i+1],weight=1,color='cyan')\n",
        "    elif i<= 7 :\n",
        "        G.add_edge(row1[i],row2[i-1],weight=1,color='cyan')\n",
        "        G.add_edge(row1[i],row2[i],weight=1,color='cyan')\n",
        "        G.add_edge(row1[i],row2[i+1],weight=1,color='cyan')\n",
        "    elif i==8:\n",
        "        G.add_edge(row1[i],row2[i-1],weight=1,color='cyan')\n",
        "        G.add_edge(row1[i],row2[i],weight=1,color='cyan')\n",
        "    elif i==9:\n",
        "        G.add_edge(row1[i],row2[i-1],weight=1,color='cyan')\n",
        "\n",
        "# add the next row in there -- row2 with row3\n",
        "for i in range(0, len(row2)):\n",
        "    if i==0:\n",
        "        G.add_edge(row2[i],row3[i],weight=1,color='black')\n",
        "        G.add_edge(row2[i],row3[i+1],weight=1,color='black')\n",
        "    elif i<= 5 :\n",
        "        G.add_edge(row2[i],row3[i-1],weight=1,color='black')\n",
        "        G.add_edge(row2[i],row3[i],weight=1,color='black')\n",
        "        G.add_edge(row2[i],row3[i+1],weight=1,color='black')\n",
        "    elif i==6:\n",
        "        G.add_edge(row2[i],row3[i-1],weight=1,color='black')\n",
        "        G.add_edge(row2[i],row3[i],weight=1,color='black')\n",
        "    elif i==7:\n",
        "        G.add_edge(row2[i],row3[i-2],weight=1,color='black')\n",
        "        G.add_edge(row2[i],row3[i-1],weight=1,color='black')\n",
        "    elif i==8:\n",
        "        G.add_edge(row2[i],row3[i-2],weight=1,color='black')\n",
        "\n",
        "# added all unidirected graphs with weights   \n",
        "\n",
        "\"\"\"\n",
        "#Good to see & possibly debug the structure of the graph as follows -\n",
        "G.number_of_nodes()\n",
        "G.number_of_edges()\n",
        "list_of_all_nodes=[row1,row2,row3]\n",
        "#flatten all_nodes as a single list - of which we can find the length, and see all edges together\n",
        "list_of_all_nodes=list(chain.from_iterable(all_nodes))\n",
        "\"\"\"\n",
        "\n",
        "edgelist1=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='red']\n",
        "edgelist2=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='blue']\n",
        "edgelist3=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='green']\n",
        "edgelist4=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='black']\n",
        "edgelist5=[(u,v) for (u,v,y) in G.edges(data=True) if y['color']=='cyan']\n",
        "\n",
        "pos=nx.spring_layout(G)\n",
        "#color coding the rows of the keyboard differently, and picking up the labels of the nodes\n",
        "nx.draw_networkx_nodes(G,pos,nodelist=row1,node_color='red',node_size=400)\n",
        "nx.draw_networkx_nodes(G,pos,nodelist=row2,node_color='blue',node_size=400)\n",
        "nx.draw_networkx_nodes(G,pos,nodelist=row3,node_color='green',node_size=400)\n",
        "\n",
        "nx.draw_networkx_edges(G,pos,edgelist1,width=4,edge_color='red')\n",
        "nx.draw_networkx_edges(G,pos,edgelist2,width=4,edge_color='blue')\n",
        "nx.draw_networkx_edges(G,pos,edgelist3,width=4,edge_color='green')\n",
        "nx.draw_networkx_edges(G,pos,edgelist4,width=3,edge_color='black')\n",
        "nx.draw_networkx_edges(G,pos,edgelist5,width=3,edge_color='cyan')\n",
        "\n",
        "# labels\n",
        "nx.draw_networkx_labels(G,pos,font_size=15,font_family='calibri')\n",
        "\n",
        "plt.axis('off')\n",
        "plt.savefig(\"finger_keying.png\")\n",
        "#plt.show() # display the QWERTY represntation graph \n",
        "\n",
        "\"\"\"\n",
        "Base assumptions for the algorithm \n",
        "1. Two words have more probabilty to be similar if the difference of their lengths does not exceed a threshold - we take this threshold as 1/3rd of the minimum of the two words keyed in\n",
        "2. Will use the LevenshteinDistance if the differnce in lenghts exceeds a threshold - meaning it must not be a typo, and we must find the regular distance metric\n",
        "\"\"\"\n",
        "\n",
        "connected_node_list=G.edges()\n",
        "\n",
        "def Qwerty_dist(word1,word2):\n",
        "    dist=0\n",
        "    number_passes_adjacency=0\n",
        "    number_passes_direction=0\n",
        "    # It can't be that most of the characters of two words are mistyped- so we prune the results, by setting another threshold for hwo many ignores are possible\n",
        "    threshold=(min(len(word1),len(word2)))/3\n",
        "    number_passes_direction_threshold= (min(len(word1),len(word2)))/2\n",
        "    number_passes_adjacency_threshold= (min(len(word1),len(word2)))/2\n",
        "    if abs(len(word1)-len(word2))<=threshold:\n",
        "        for i in range(0,min(len(word1),len(word2))):\n",
        "            if word1[i]==word2[i]:\n",
        "                pass\n",
        "            elif (word1[i],word2[i]) or (word2[i],word1[i]) in connected_node_list :\n",
        "                #meaning the intention of the user must have been same and it's indeed a typo\n",
        "                number_passes_adjacency=number_passes_adjacency+1\n",
        "                #pass\n",
        "            elif (word1[i],word2[i]) or (word2[i],word1[i]) not in connected_node_list :\n",
        "            #looks like the characters in the words just got reversed, we place the restriction that the two swapped \"keys\" happen to come from left & right section of keyboard\n",
        "                if i<=min(len(word1),len(word2))-2 :\n",
        "                    if (word1[i] in left_keys and word2[i] in right_keys) or (word1[i] in right_keys and word2[i] in left_keys):\n",
        "                        #either the keys are exactly reversed \n",
        "                        if word1[i]==word2[i+1] and word1[i+1]==word2[i]:\n",
        "                            number_passes_direction=number_passes_direction+1\n",
        "                            #pass\n",
        "                        #otherwise, it could be that the \"swapped\" words are \"neighborhood\" connected, hence looking up in connected_node_list- ie we swapped, but we did a typo !\n",
        "                        elif (word1[i],word2[i+1]) and (word1[i+1],word2[i]) in connected_node_list :\n",
        "                            number_passes_adjacency=number_passes_adjacency+1\n",
        "                            #pass\n",
        "                        else :\n",
        "                            dist = dist+1\n",
        "            else: \n",
        "                dist=dist+1\n",
        "        if dist ==0 and number_passes_direction <= number_passes_direction_threshold  and number_passes_adjacency <= number_passes_adjacency_threshold :\n",
        "            return 0;\n",
        "        elif dist>0 and number_passes_direction <= number_passes_direction_threshold  and number_passes_adjacency <= number_passes_adjacency_threshold :\n",
        "            return dist;\n",
        "        elif number_passes_direction > number_passes_direction_threshold  or number_passes_adjacency > number_passes_adjacency_threshold :\n",
        "            dist=editDistDP(word1,word2)\n",
        "            return dist;\n",
        "    else:\n",
        "    #if abs(len(word1)-len(word2))>threshold :\n",
        "        dist=editDistDP(word1,word2)\n",
        "        return dist;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPaZu3V8T6Ys"
      },
      "source": [
        "corrected_queries_qwerty = {}\n",
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['Query IDs', 'Typo', 'Term', 'Distance'])\n",
        "for i in spell_error_queries.keys():\n",
        "  corrected_queries_qwerty[i] = []\n",
        "  for word in spell_error_queries[i]:\n",
        "    mini = 5\n",
        "    corr_word = word\n",
        "    listRow = [];\n",
        "    listRow.append(i);\n",
        "    listRow.append(word);\n",
        "    for original_word in myInvertedIndex2.final_dict.keys():\n",
        "      if original_word in STOP_WORDS:\n",
        "        continue\n",
        "      temp = Qwerty_dist(word, original_word)\n",
        "      if (temp<mini):\n",
        "        mini = temp\n",
        "        corr_word = original_word\n",
        "    listRow.append(corr_word);\n",
        "    listRow.append(mini);\n",
        "    corrected_queries_qwerty[i].append(corr_word)\n",
        "    if corr_word != word:\n",
        "      t.add_row(listRow);\n",
        "print(t);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}